{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MP Assignment Behavior Cloning\n",
    "\n",
    "Thao Dang, HS Esslingen\n",
    "\n",
    "![Gymnasium car racing environment](https://gymnasium.farama.org/_images/car_racing.gif)\n",
    "\n",
    "In this assignment, you will use behavior cloning to run a car in gymnasiums car racing environment. The assignment is based on a course of Andreas Geiger at the University of Tübingen. Pytorch will be used for the behavior cloning network, so you should run this notebook with a GPU.\n",
    "To enable GPU's in colab, select \n",
    "```bash\n",
    "\"Runtime\" > “Change runtime type” > \"T4 GPU\"\n",
    "```\n",
    "after you uploaded the notebook file to colab.\n",
    "\n",
    "We will use the discrete version of the car racing environment. A race car should drive a random environment as fast as possible, where potential discrete actions comprise (a combination of)\n",
    "- do nothing\n",
    "- steer left\n",
    "- steer right\n",
    "- gas\n",
    "- brake\n",
    "\n",
    "Expert demonstrations are provided, though you can later record your own demos as an optional exercise (but not in colab!). The assignemt is structured as follows:\n",
    "1. Test gymnasium environment\n",
    "2. Analyse the expert demonstrations\n",
    "3. Design and train the behavior cloning network with pytorch\n",
    "4. Evaluate the network\n",
    "5. Optional extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test gymnasium environment\n",
    "\n",
    "First, upload the required additional files to colab (file symbol icon on the left side bar):\n",
    "- utils.py \n",
    "- demos_no_braking.npz\n",
    "\n",
    "Then, install the necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "!pip install swig\n",
    "!pip install moviepy\n",
    "!pip install gymnasium[box2d]\n",
    "!pip install graphviz\n",
    "!pip install torchview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import time\n",
    "from dataclasses import dataclass, field\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from utils import *\n",
    "\n",
    "# torch.use_deterministic_algorithms(True)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "# The flag below controls whether to allow TF32 on matmul. This flag defaults to True.\n",
    "torch.backends.cuda.matmul.allow_tf32 = False\n",
    "# The flag below controls whether to allow TF32 on cuDNN. This flag defaults to True.\n",
    "torch.backends.cudnn.allow_tf32 = False\n",
    "\n",
    "# remove old video files (if any)\n",
    "if os.path.exists('output'):\n",
    "    os.system('rm -f output/*.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test gymnasium environment\n",
    "\n",
    "The following code starts the car racing environment, runs some random actions, and creates a video of the trial. Make sure that everything is installed properly and the video is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SDC_Wrapper(gym.make(\"CarRacing-v3\", render_mode = \"rgb_array\"), remove_score=True)\n",
    "env = RenderFrame(env, \"./output\")\n",
    "\n",
    "observation, info = env.reset()\n",
    "\n",
    "while True:\n",
    "  action = env.action_space.sample()\n",
    "  observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "  if terminated or truncated:\n",
    "    break\n",
    "\n",
    "env.play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect training data\n",
    "\n",
    "First, we need to have a look at the expert data. The following code loads the data as observations (i.e., the training features) and actions (i.e., our targets).\n",
    "\n",
    "1. Visualize samples of the features (using ``plt.imshow(...)``) and the corresponding actions.\n",
    "\n",
    "2. The actions ``action=[steering, acceleration, braking]`` only contain discrete values. Find out which values for the actions occur in the dataset (using ``np.unique``)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = ['demos_no_braking.npz']\n",
    "observations, actions = load_demonstrations(data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Visualize the data\n",
    "# 2. Check the range of the actions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The actions you have analyzed above are not independent (e.g. accelerate and brake simultaneously does not make sense). So, it's useful to define classes of possible actions, like {steer left}, {steer left and accelerate}, {accelerate}, etc. Based on the numerical values from the previous task, define a full set of discrete action classes, e.g.\n",
    "\n",
    "```python\n",
    "    CLASSES = [\n",
    "            [-1.0, 0.0, 0.0],  # 0: Left\n",
    "            [-1.0, 0.5, 0.0],  # 1: left with acceleration\n",
    "            [0.0, 0.0, 0.0],   # 2: straight\n",
    "            ...\n",
    "        ]\n",
    "```\n",
    "\n",
    "4. Count how many instances of each class are present in the data set. Is the data set well-balanced? What might be the consequences of this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Define action classes\n",
    "# 4. Check the distribution of the action classes\n",
    "\n",
    "CLASSES = [\n",
    "            [-1.0, 0.0, 0.0],  # 0: Left\n",
    "            [-1.0, 0.5, 0.0],  # 1: left with acceleration\n",
    "            # ...\n",
    "        ]\n",
    "\n",
    "print('Classes:')\n",
    "print(np.array(CLASSES))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Neural Network\n",
    "\n",
    "1. Design a small first network architecture in the ``ClassificationNetwork`` below. Start with two to three convolutional layers, \n",
    "followed by two or three fully connected layers (linear layers) to extract the output vector. Let each layer be followed by a ReLU as the non-linear activation (see code\n",
    "snippets below). Add your code in the ``__init__`` and ``forward`` function.\n",
    "\n",
    "    ```python\n",
    "    self.cnn_layers = torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(in_channels, out_channels, filter_size, stride),\n",
    "        torch.nn.LeakyReLU(negative_slope=0.2))\n",
    "\n",
    "    self.fcn_layers = torch.nn.Sequential(\n",
    "        torch.nn.Linear(in_size, out_size),\n",
    "        torch.nn.LeakyReLU(negative_slope=0.2))\n",
    "    ````\n",
    "\n",
    "2. You can visualize your network architecture with the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationNetwork(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Implementation of the network layers. The image size of the input\n",
    "        observations is 96x96 pixels.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # setting device on GPU if available, else CPU\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        self.classes = CLASSES # as defined above\n",
    "        \n",
    "        # DEFINE YOUR LAYERS HERE\n",
    "        # ...\n",
    "\n",
    "\n",
    "    def forward(self, observation):\n",
    "        \"\"\"\n",
    "        The forward pass of the network. Returns the prediction for the given\n",
    "        input observation.\n",
    "        observation:   torch.Tensor of size (batch_size, 96, 96, 3)\n",
    "        return         torch.Tensor of size (batch_size, C)\n",
    "        \"\"\"\n",
    "        batch_size = observation.shape[0]\n",
    "\n",
    "        # INSERT YOUR CODE HERE\n",
    "        obs = observation.reshape(batch_size, 3, 96, 96)\n",
    "        # ...\n",
    "        \n",
    "\n",
    "    def actions_to_classes(self, actions):\n",
    "        \"\"\"\n",
    "        For a given set of actions map every action to its corresponding\n",
    "        action-class representation. Every action is represented by a 1-dim vector\n",
    "        with the entry corresponding to the class number.\n",
    "        actions:        python list of N torch.Tensors of size 3\n",
    "        return          python list of N torch.Tensors of size 1\n",
    "        \"\"\"\n",
    "        return [\n",
    "            torch.Tensor(\n",
    "                [\n",
    "                    int(\n",
    "                        torch.prod(action == this_class)\n",
    "                    )  # calculate the product of the elements in boolean tensor and converts them to a list\n",
    "                    for this_class in torch.Tensor(self.classes)\n",
    "                ]\n",
    "            )\n",
    "            for action in actions\n",
    "        ]\n",
    "\n",
    "    def scores_to_action(self, scores):\n",
    "        \"\"\"\n",
    "        Maps the scores predicted by the network to an action-class and returns\n",
    "        the corresponding action [steer, gas, brake].\n",
    "                        C = number of classes\n",
    "        scores:         python list of torch.Tensors of size C\n",
    "        return          (float, float, float)\n",
    "        \"\"\"\n",
    "        x, class_number = torch.max(\n",
    "            scores[0], dim=0\n",
    "        )  # Finds the max from the 9X1 column vector\n",
    "        steer, gas, brake = self.classes[\n",
    "            class_number\n",
    "        ]  # Uses the classes you defined in init function to get back the appropriate values for steer,gas,brake\n",
    "        return steer, gas, brake\n",
    "\n",
    "    def extract_sensor_values(self, observation, batch_size):\n",
    "        # just approximately normalized, usually this suffices.\n",
    "        # can be changed by you\n",
    "        speed_crop = observation[:, 84:94, 12, 0].reshape(batch_size, -1)\n",
    "        speed = speed_crop.sum(dim=1, keepdim=True) / 255 / 5\n",
    "\n",
    "        abs_crop = observation[:, 84:94, 18:25:2, 2].reshape(batch_size, 10, 4)\n",
    "        abs_sensors = abs_crop.sum(dim=1) / 255 / 5\n",
    "\n",
    "        steer_crop = observation[:, 88, 38:58, 1].reshape(batch_size, -1) / 255 / 10\n",
    "        steer_crop[:, :10] *= -1\n",
    "        steering = steer_crop.sum(dim=1, keepdim=True)\n",
    "\n",
    "        gyro_crop = observation[:, 88, 58:86, 0].reshape(batch_size, -1) / 255 / 5\n",
    "        gyro_crop[:, :14] *= -1\n",
    "        gyroscope = gyro_crop.sum(dim=1, keepdim=True)\n",
    "\n",
    "        return speed, abs_sensors.reshape(batch_size, 4), steering, gyroscope\n",
    "    \n",
    "    \n",
    "model = ClassificationNetwork()\n",
    "print(model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchview import draw_graph\n",
    "import graphviz\n",
    "\n",
    "# when running on VSCode run the below command\n",
    "# svg format on vscode does not give desired result\n",
    "graphviz.format = 'png'\n",
    "model_graph1 = draw_graph(ClassificationNetwork(), input_size=(128, 96, 96, 3))\n",
    "model_graph1.visual_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training\n",
    "\n",
    "1. Read and modify the ``training`` function. Use the torch ``CrossEntropyLoss`` (combines LogSoftmax and NLLLoss - negative log likelihood loss)\n",
    "to calculate the loss. Adjust the number of classes you defined above.\n",
    "\n",
    "2. You can evaluate the results with the two cells in the Evaluation chapter below. ``evaluate`` generates a video of a random scenario. ``calculate_score``computes a mean score over a fixed set of scenarios.\n",
    "\n",
    "3. Now you can do some hyperparameter optimization. What are learning rate, batch size, and number of epochs? Can you get better performance with other values?\n",
    "You may change the network architecture and use maybe dropout or batch normalization. In addition, you may set class weights in the ``CrossEntropyLoss`` function to counter potentially unbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DefaultArguments:\n",
    "    \"\"\"Genereal arguments for our training.\"\"\"\n",
    "    agent_load_path: str = \"agent.pth\" # Path to the .pth file of the trained agent\n",
    "    agent_save_path: str = \"agent.pth\" # Save path of the trained model\n",
    "    training_data_path: list = field(default_factory=lambda: [\"demos_no_braking.npz\"]) # training data files\n",
    "    lr: float = 1e-3 #1e-2 # Learning rate\n",
    "    batch_size: int = 128 # Batch size\n",
    "    nr_epochs: int = 75 # Number of training epochs\n",
    "    no_display: bool = False # disable video output\n",
    "\n",
    "\n",
    "def train(data_file, trained_network_file, args):\n",
    "    \"\"\"\n",
    "    Function for training the network.\n",
    "    \"\"\"\n",
    "    infer_action = ClassificationNetwork()\n",
    "    optimizer = torch.optim.Adam(infer_action.parameters(), lr=args.lr)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Loss function\n",
    "    # MODIFY CODE HERE    \n",
    "    loss_function = ...\n",
    "\n",
    "    observations, actions = load_demonstrations(data_file)\n",
    "    observations = [torch.Tensor(observation) for observation in observations]\n",
    "    actions = [torch.Tensor(action) for action in actions]\n",
    "\n",
    "    batches = [batch for batch in zip(observations,\n",
    "                                      infer_action.actions_to_classes(actions))]\n",
    "\n",
    "    # setting device on GPU if available, else CPU\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    nr_epochs = args.nr_epochs\n",
    "    batch_size = args.batch_size\n",
    "    number_of_classes = 9  # NEEDS TO BE ADJUSTED\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(nr_epochs):\n",
    "        random.shuffle(batches)\n",
    "\n",
    "        total_loss = 0\n",
    "        batch_in = []\n",
    "        batch_gt = []\n",
    "        for batch_idx, batch in enumerate(batches):\n",
    "            batch_in.append(batch[0].to(device))\n",
    "            batch_gt.append(batch[1].to(device))\n",
    "\n",
    "            if (batch_idx + 1) % batch_size == 0 or batch_idx == len(batches) - 1:\n",
    "                batch_in = torch.reshape(torch.cat(batch_in, dim=0),\n",
    "                                         (-1, 96, 96, 3))\n",
    "                batch_gt = torch.reshape(torch.cat(batch_gt, dim=0), (-1, number_of_classes))\n",
    "\n",
    "                batch_out = infer_action(batch_in)\n",
    "                loss = loss_function(batch_out, batch_gt)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss\n",
    "\n",
    "                batch_in = []\n",
    "                batch_gt = []\n",
    "\n",
    "        time_per_epoch = (time.time() - start_time) / (epoch + 1)\n",
    "        time_left = (1.0 * time_per_epoch) * (nr_epochs - 1 - epoch)\n",
    "        print(\"Epoch %5d\\t[Train]\\tloss: %.6f \\tETA: +%fs\" % (\n",
    "            epoch + 1, total_loss, time_left))\n",
    "\n",
    "    torch.save(infer_action, trained_network_file)\n",
    "\n",
    "\n",
    "args = DefaultArguments()\n",
    "train(args.training_data_path, args.agent_save_path, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(args, trained_network_file):\n",
    "    \"\"\"\n",
    "    evaluate the trained network on random environment and create video\n",
    "    \"\"\"\n",
    "    # setting device on GPU if available, else CPU\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    infer_action = torch.load(trained_network_file, map_location=device)\n",
    "    infer_action.eval()\n",
    "\n",
    "    env = SDC_Wrapper(gym.make(\"CarRacing-v3\", render_mode = \"rgb_array\"), remove_score=True)\n",
    "    env = RenderFrame(env, \"./output\", auto_release=False)\n",
    "    observation, info = env.reset()\n",
    "\n",
    "    reward_per_episode = 0\n",
    "    for t in range(600):\n",
    "        action_scores = infer_action(torch.Tensor(\n",
    "            np.ascontiguousarray(observation[None])).to(device))\n",
    "\n",
    "        action = infer_action.scores_to_action(action_scores)\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        reward_per_episode += reward\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    print('total reward %f' % (reward_per_episode))\n",
    "    env.release()\n",
    "    env.play()\n",
    "\n",
    "    \n",
    "evaluate(args, 'agent.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score(args, trained_network_file):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of the network on some standard test environments.\n",
    "    \"\"\"\n",
    "    # setting device on GPU if available, else CPU\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    infer_action = torch.load(trained_network_file, map_location=device)\n",
    "    infer_action.eval()\n",
    "\n",
    "    env = SDC_Wrapper(gym.make('CarRacing-v3', render_mode='human'), remove_score=True)\n",
    "\n",
    "    seeds = [22597174, 68545857, 75568192, 91140053, 86018367,\n",
    "             49636746, 66759182, 91294619, 84274995, 31531469]\n",
    "\n",
    "    total_reward = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for episode, seed in enumerate(seeds):\n",
    "            observation, info = env.reset(seed=seed)\n",
    "            reward_per_episode = 0\n",
    "            for t in tqdm(range(600), leave=False):\n",
    "                action_scores = infer_action(torch.Tensor(\n",
    "                    np.ascontiguousarray(observation[None])).to(device))\n",
    "\n",
    "                steer, gas, brake = infer_action.scores_to_action(action_scores)\n",
    "                observation, reward, terminated, truncated, info = env.step([steer, gas, brake])\n",
    "                reward_per_episode += reward\n",
    "                \n",
    "                if terminated or truncated:\n",
    "                    break\n",
    "\n",
    "            print('episode %d \\t reward %f' % (episode, reward_per_episode))\n",
    "            total_reward += np.clip(reward_per_episode, 0, np.inf)\n",
    "\n",
    "    print('---------------------------')\n",
    "    print(' total score: %f' % (total_reward / len(seeds)))\n",
    "    print('---------------------------')\n",
    "    env.close()\n",
    "    \n",
    "\n",
    "calculate_score(args, 'agent.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional extensions and improvements\n",
    "\n",
    "There is no definite solution to this assignment and there are many ways in which the results could potentially be improved. Here are some suggestions you could try:\n",
    "\n",
    "1. There is a function ``extract_sensor_values`` that retrieves some additional values from the image. Check what this function is doing and include its results as additionals features in the network. How does the performance of the network change?\n",
    "\n",
    "2. Data augmentation: A common technique in deep learning is the synthetic generation of additional data by augmenting the existing (observation, action) tuples? What augmentation methods can you think of? Implement them. Does the performance change?\n",
    "\n",
    "3. Record your own data and rerun training. Check the files ``record_demonstrations.py`` and ``environment.yml`` to do this (requires Anaconda on your own PC since recording cannot be run in a jupyter notebook).\n",
    "\n",
    "4. Formulate the problem as a regression problem instead of a classification problem. You can change the setup by adding the option ``continuous=False`` in the ``gym.make`` command and a lot of other modifications in the notebook and training script.\n",
    "\n",
    "5. ... and a lot more... If you find very good solutions or solutions that you're proud of, please send them to me!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "behavior-cloning",
   "language": "python",
   "name": "behavior-cloning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
