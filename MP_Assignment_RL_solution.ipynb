{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_bRzdowadou"
   },
   "source": [
    "# Motion Planning, Assignment 8: Q-Learning with openai gym\n",
    "\n",
    "Thao Dang, Hochschule Esslingen\n",
    "\n",
    "In this assignment you will experiment with Q learning for a classic control problem: balancing a pole on a moving cart.\n",
    "\n",
    "![cartpole.png](https://drive.google.com/uc?id=1b9GxE9zRkxPCBX-D-LfrIrbehNZdjP4J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8s23mvXNadoz"
   },
   "source": [
    "Note for solution: \n",
    "This notebook is based on https://github.com/IsaacPatole/CartPole-v0-using-Q-learning-SARSA-and-DNN/blob/master/Qlearning_for_cartpole.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C-1AWMGQado0"
   },
   "source": [
    "# Simulations with openai gym\n",
    "\n",
    "This assignment will utilize a famous simulation environment for testing reinforcement learning algorithms: http://gym.openai.com\n",
    "\n",
    "## Installation\n",
    "\n",
    "If you are using Anaconda or Miniconda and a **Mac or a Unix machine**, installation is rather straightforward. Just create the following environment and you should be good to go:\n",
    "```\n",
    "    conda create --name gym python=3.7\n",
    "    conda activate gym\n",
    "\n",
    "    conda install conda matplotlib seaborn\n",
    "    conda install -c conda-forge jupyter_contrib_nbextensions gym-all\n",
    "    conda install -c anaconda tensorflow\n",
    "    \n",
    "    jupyter notebook\n",
    "```\n",
    "\n",
    "If you are using **Windows**, you may have to additionally have to install other libraries as described [here](https://towardsdatascience.com/how-to-install-openai-gym-in-a-windows-environment-338969e24d30) (but I haven't tried this and can't support).\n",
    "\n",
    "When running in **Colab**, just transform the following cell to code and execute it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8K6hNRX4ado0"
   },
   "source": [
    "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
    "\n",
    "!apt-get update\n",
    "\n",
    "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m4MmSLgwado0"
   },
   "source": [
    "Now, let's import all relevant packages and define some import functions that help displaying videos on colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "238zytHoado0"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "SuYK46Ipado1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not in Colab.\n"
     ]
    }
   ],
   "source": [
    "IN_COLAB = False\n",
    "try:\n",
    "    from google.colab import files\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    print('Not in Colab.')\n",
    "\n",
    "    \n",
    "if IN_COLAB:\n",
    "    from IPython.display import HTML\n",
    "    from pyvirtualdisplay import Display\n",
    "    from IPython import display as ipythondisplay\n",
    "\n",
    "    display = Display(visible=0, size=(1400, 900))\n",
    "    display.start()\n",
    "\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "Utility functions to enable video recording of gym environment \n",
    "and displaying it.\n",
    "To enable video, just do \"env = wrap_env(env)\"\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def show_video():\n",
    "    if not IN_COLAB:\n",
    "        return\n",
    "    \n",
    "    # in colab, everything is done via video\n",
    "    mp4list = glob.glob('video/*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "                </video>'''.format(encoded.decode('ascii'))))\n",
    "    else: \n",
    "        print(\"Could not find video\")\n",
    "    \n",
    "\n",
    "def wrap_env(env):\n",
    "    env = Monitor(env, './video', force=True)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NqlpqrwUado2"
   },
   "source": [
    "## The cart pole environment\n",
    "\n",
    "After everything has been set up, we are ready to try some nice simulations. Here, we will only use the cart pole environment (https://gym.openai.com/envs/CartPole-v1/). The description of this environment is as follows:\n",
    "```\n",
    "Description:\n",
    "    A pole is attached by an un-actuated joint to a cart, which moves along\n",
    "    a frictionless track. The pendulum starts upright, and the goal is to\n",
    "    prevent it from falling over by increasing and reducing the cart's\n",
    "    velocity.\n",
    "Source:\n",
    "    This environment corresponds to the version of the cart-pole problem\n",
    "    described by Barto, Sutton, and Anderson\n",
    "Observation:\n",
    "    Type: Box(4)\n",
    "    Num     Observation               Min                     Max\n",
    "    0       Cart Position             -4.8                    4.8\n",
    "    1       Cart Velocity             -Inf                    Inf\n",
    "    2       Pole Angle                -0.418 rad (-24 deg)    0.418 rad (24 deg)\n",
    "    3       Pole Angular Velocity     -Inf                    Inf\n",
    "Actions:\n",
    "    Type: Discrete(2)\n",
    "    Num   Action\n",
    "    0     Push cart to the left\n",
    "    1     Push cart to the right\n",
    "    Note: The amount the velocity that is reduced or increased is not\n",
    "    fixed; it depends on the angle the pole is pointing. This is because\n",
    "    the center of gravity of the pole increases the amount of energy needed\n",
    "    to move the cart underneath it\n",
    "Reward:\n",
    "    Reward is 1 for every step taken, including the termination step\n",
    "Starting State:\n",
    "    All observations are assigned a uniform random value in [-0.05..0.05]\n",
    "Episode Termination:\n",
    "    Pole Angle is more than 12 degrees.\n",
    "    Cart Position is more than 2.4 (center of the cart reaches the edge of\n",
    "    the display).\n",
    "    Episode length is greater than 200.\n",
    "    Solved Requirements:\n",
    "    Considered solved when the average return is greater than or equal to\n",
    "    195.0 over 100 consecutive trials.    \n",
    "```\n",
    "(taken from: https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SCv7Wq_dado3"
   },
   "source": [
    "To run this simulation, the following class ``CartPoleBase`` may be useful. It provides two important functions:\n",
    "* ``act``: the standard policy function that generates an action from the current state data (you will overwrite this function later), and\n",
    "* ``run``: which runs and renders a simulation once ``act`` has been defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nCa40ihBado3"
   },
   "outputs": [],
   "source": [
    "class CartPoleBase():\n",
    "    def __init__(self, num_episodes=500):\n",
    "        \"\"\"\n",
    "        The constructor.\n",
    "        num_episodes is the max number of steps to take (given that the trial does not fail before)\n",
    "        \"\"\"\n",
    "        self.num_episodes = num_episodes\n",
    "        self.env = gym.make('CartPole-v0')\n",
    "        \n",
    "        \n",
    "    def act(self, observation, reward, done):\n",
    "        \"\"\"\n",
    "        A very simple agent that randomly chooses an action.\n",
    "        \n",
    "        Arguments (everything the simulator provides): \n",
    "            observation: the observed state (Cart Position, Cart Velocity, Pole Angle, \n",
    "                         Pole Angular Velocity)\n",
    "            reward:      1 for every step taken\n",
    "            done:        true if simulation terminated\n",
    "    \n",
    "        Return:\n",
    "            0 to push cart to the left, 1 to push cart to the right\n",
    "        \"\"\"\n",
    "        assert False, \"not implemented yet\"\n",
    "        return 0 \n",
    "        \n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Runs an episode while displaying the cartpole environment.\"\"\"\n",
    "        self.env = wrap_env(self.env)\n",
    "        t = 0\n",
    "        reward = 0\n",
    "        done = False\n",
    "        obs = self.env.reset()\n",
    "        \n",
    "        while not done:\n",
    "                self.env.render()\n",
    "                t = t+1\n",
    "                action = self.act(obs, reward, done)\n",
    "                obs, reward, done, _ = self.env.step(action)\n",
    "                \n",
    "        self.env.close()\n",
    "        show_video()\n",
    "        return t\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4ItpmPDado3"
   },
   "source": [
    "# Exercise 1: Random policy\n",
    "\n",
    "Let's first try everything we have installed so far by running an agent that chooses actions at random and render the scene. You will only have to add one line here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "wNNPPtYlado3"
   },
   "outputs": [],
   "source": [
    "class CartPoleRandom(CartPoleBase):\n",
    "    def act(self, observation, reward, done):\n",
    "        \"\"\"\n",
    "        A very simple agent that randomly chooses an action.\n",
    "        \n",
    "        Arguments (everything the simulator provides): \n",
    "            observation: the observed state (Cart Position, Cart Velocity, Pole Angle, \n",
    "                         Pole Angular Velocity)\n",
    "            reward:      1 for every step taken\n",
    "            done:        true if simulation terminated\n",
    "    \n",
    "        Return:\n",
    "            0 to push cart to the left, 1 to push cart to the right\n",
    "        \"\"\"\n",
    "        ## TODO: Add your code here \n",
    "        return self.env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 438
    },
    "id": "kh0d5SX1ado4",
    "outputId": "7c8c632f-d646-43c3-da1a-89ee0efc7420"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished after 45 steps.\n"
     ]
    }
   ],
   "source": [
    "randomAgent = CartPoleRandom()\n",
    "steps_taken = randomAgent.run()\n",
    "print('Finished after %d steps.' % steps_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1UXPXWmQado4"
   },
   "source": [
    "# Exercise 2: Q-learning\n",
    "\n",
    "Next, you will use Q-learning based on the Bellmann equation as described in the lecture to find a **Q-value function** or **Q-table** that estimates the maximum expected cumulative reward achievable\n",
    "from a given (state, action) pair.\n",
    "\n",
    "A Q-table is discrete in its nature, yet the problem we are considering here is continous since the observations that we make (cart position, cart velocity, pole angle, pole angular velocity) are continous, real numbers. To still be able to use Q-learning, we need to **discretize** our system. This is done by quantisation of all observations in separate buckets (or bins) and refering to them by the index of the bucket in which they belong.\n",
    "E.g. if we know that cart velocity is always within the range -0.5..0.5 and we want to use four bins for discretization, we could transform the real-valued cart velocity $v$ to a discrete value $v_d=0$ if $v < -0.25$, to $v_d=1$ if $-0.25 \\leq v < 0$, to $v_d=2$ if $0 \\leq v < 0.25$, and to $v_d=3$ if $v \\geq 0.5$. Using such a discretization, our cart pole problem becomes computationally tractable for the Q-learning algorithm.  \n",
    "\n",
    "The Q-learning algorithm that you will implement here is a slightly modified version from the algorithm given in the lecture:\n",
    "\n",
    "> **Given**: rewards $r$, discount factor $\\gamma$, learning rate $\\alpha$, probability threshold $\\epsilon$\n",
    ">\n",
    "> **Algorithm**:\n",
    "> 1. Initialize Q-table to zero.\n",
    "> 2. For each episode (a.k.a. training session):\n",
    ">> 3. Select (random) initial state $s_t$.\n",
    ">> 4. Do while the goal state hasn't been reached:\n",
    ">>> 5. If random number $p<\\epsilon$: Randomly select one action $a_t$ among all possible actions for the current state $s_t$. Otherwise: Select best action based on current Q-table. Both actions result in next state $s_{t+1}$.\n",
    ">>> 6. Update expected cumulative reward: $$Q(s_t, a_t) \\leftarrow (1âˆ’\\alpha) ð‘„(s_t, a_t) + \\alpha \\left[ r(s_t, a_t) + \\gamma max_{a}â¡ \\{ Q(s_{t+1}, a) \\} \\right]$$\n",
    ">>> 7. Set the next state as the current state $s_t \\leftarrow s_{t+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "leGTPpEUado4"
   },
   "source": [
    "## Exercise 2.1: Algorithm\n",
    "\n",
    "What is the major difference between the algorithm shown above and the version presented in the lecture. What are potential reasons behind this modification?\n",
    "\n",
    "In the algorithm above, it is common to adapt the learning and epsilon with the episodes. How and why would you modify these parameters? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YbQF7PKvado5"
   },
   "source": [
    "The following class is a skeleton for implemening the Q-learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "q_HXIjOEado5"
   },
   "outputs": [],
   "source": [
    "class CartPoleQAgent(CartPoleBase):\n",
    "    def __init__(self, num_bins=(3, 3, 6, 6), \n",
    "                 num_episodes=500, min_lr=0.1, \n",
    "                 min_epsilon=0.1, discount=1.0, decay=25):\n",
    "        \"\"\"\n",
    "        The constructor.\n",
    "        Arguments:\n",
    "            num_bins: the number of bins for discretization of all four observations\n",
    "            num_episodes: max number of steps to take \n",
    "            min_lr: minimum learning rate\n",
    "            min_epsilon: minimum epsilon value (used for sampling random actions)\n",
    "            discount: discount factor\n",
    "            decay: a decay factor used for adaptation of the learning rate and epsilon\n",
    "        \"\"\"\n",
    "        CartPoleBase.__init__(self, num_episodes)\n",
    "        \n",
    "        self.num_bins = num_bins\n",
    "        self.min_lr = min_lr\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.discount = discount\n",
    "        self.decay = decay\n",
    "        \n",
    "        # action-value function initialized with 0's\n",
    "        # the Q-table has shape \"num_bins[0] x num_bins[1] x num_bins[2] x num_bins[3] x 2\"\n",
    "        self.Q_table = np.zeros(self.num_bins + (self.env.action_space.n,))\n",
    "\n",
    "        # [position, velocity, angle, angular velocity]\n",
    "        self.upper_bounds = [self.env.observation_space.high[0], 0.5, self.env.observation_space.high[2], math.radians(50) / 1.]\n",
    "        self.lower_bounds = [self.env.observation_space.low[0], -0.5, self.env.observation_space.low[2], -math.radians(50) / 1.]\n",
    "        \n",
    "        # bins for discretization\n",
    "        self.bins = []\n",
    "        for low, high, num in zip(self.lower_bounds, self.upper_bounds, self.num_bins):\n",
    "            delta = (high-low)/num\n",
    "            b = [low+(i+1)*delta for i in range(num-1)]\n",
    "            self.bins.append(b)\n",
    "        \n",
    "        # book keeping of steks take in each episode\n",
    "        self.steps = np.zeros(self.num_episodes)\n",
    "        \n",
    "\n",
    "    def discretize_state(self, obs):\n",
    "        \"\"\"\n",
    "        Takes an observation of the environment and aliases it.\n",
    "        By doing this, very similar observations can be treated\n",
    "        as the same and it reduces the state space so that the \n",
    "        Q-table can be smaller and more easily filled.\n",
    "        \n",
    "        Input:\n",
    "        obs (tuple): Tuple containing 4 floats describing the current\n",
    "                     state of the environment.\n",
    "        \n",
    "        Output:\n",
    "        discretized (tuple): Tuple containing 4 non-negative integers smaller \n",
    "                             than n where n is the number in the same position\n",
    "                             in the num_bins list.\n",
    "        \"\"\"\n",
    "        discretized = list()\n",
    "        for i in range(len(obs)):\n",
    "            new_obs = np.digitize(obs[i], self.bins[i])\n",
    "            discretized.append(new_obs)\n",
    "        \n",
    "        return tuple(discretized)\n",
    "\n",
    "\n",
    "    def decay_function(self, t, min_val):\n",
    "        \"\"\"\n",
    "        A decay function for modelling learning rate and epsilon in training function.\n",
    "        Arguments:\n",
    "            t: num epochs\n",
    "            min_val: returned decay value is never smaller than this\n",
    "        \"\"\"\n",
    "        return max(min_val, min(1., 1. - math.log10((t + 1) / self.decay)))\n",
    "\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Implements the Q-learning algorithm as described in the text above.\n",
    "        \"\"\"\n",
    "        # Looping for each episode\n",
    "        for e in range(self.num_episodes):\n",
    "            # Initializes the state\n",
    "            current_state = self.discretize_state(self.env.reset())\n",
    "\n",
    "            # decaying learning rate and epsilon \n",
    "            self.learning_rate = self.decay_function(e, self.min_lr)\n",
    "            self.epsilon = self.decay_function(e, self.min_epsilon)\n",
    "            done = False\n",
    "            \n",
    "            # Looping for each step\n",
    "            while not done:\n",
    "                self.steps[e] += 1\n",
    "                \n",
    "                ## TODO: Add your code here\n",
    "                \n",
    "                # choose action from state\n",
    "                if (np.random.random() < self.epsilon):\n",
    "                    action = self.env.action_space.sample() \n",
    "                else:\n",
    "                    action = np.argmax(self.Q_table[current_state])\n",
    "        \n",
    "                # Take action\n",
    "                obs, reward, done, _ = self.env.step(action)\n",
    "                new_state = self.discretize_state(obs)\n",
    "                \n",
    "                # Update Q(S,A)\n",
    "                self.Q_table[current_state][action] = (1-self.learning_rate) * self.Q_table[current_state][action] + \\\n",
    "                    self.learning_rate * (reward + self.discount * np.max(self.Q_table[new_state]))\n",
    "                \n",
    "                current_state = new_state\n",
    "                \n",
    "                # We break out of the loop when done is False which is\n",
    "                # a terminal state.\n",
    "        print('Finished training!')\n",
    "        \n",
    "    \n",
    "    def plot_learning(self):\n",
    "        \"\"\"\n",
    "        Plots the number of steps at each episode and prints the\n",
    "        amount of times that an episode was successfully completed.\n",
    "        \"\"\"\n",
    "        sns.lineplot(x=range(len(self.steps)), y=self.steps)\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Steps\")\n",
    "        plt.show()\n",
    "        t = 0\n",
    "        for i in range(self.num_episodes):\n",
    "            if self.steps[i] == 200:\n",
    "                t+=1\n",
    "        print(t, \" episodes were successfully completed.\")\n",
    "        \n",
    "        \n",
    "    def act(self, observation, reward, done):\n",
    "        \"\"\"\n",
    "        A Q-learning based agent.\n",
    "        \n",
    "        Arguments (everything the simulator provides): \n",
    "            observation: the observed state (Cart Position, Cart Velocity, Pole Angle, \n",
    "                         Pole Angular Velocity)\n",
    "            reward:      1 for every step taken\n",
    "            done:        true if simulation terminated\n",
    "    \n",
    "        Return:\n",
    "            0 to push cart to the left, 1 to push cart to the right\n",
    "        \"\"\"\n",
    "        \n",
    "        ## TODO: Add your code here\n",
    "        state = self.discretize_state(observation)\n",
    "        return np.argmax(self.Q_table[state])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ugkmBiKdado8"
   },
   "source": [
    "## Exercise 2.2: Training\n",
    "\n",
    "Implement the ``train`` function above and run the training using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "id": "_iqmA2Trado9",
    "outputId": "32bf65e0-0522-4a54-c8c5-ce4f5b41e826"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227  episodes were successfully completed.\n"
     ]
    }
   ],
   "source": [
    "agent = CartPoleQAgent()\n",
    "agent.train()\n",
    "agent.plot_learning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jsQFxEJhado-"
   },
   "source": [
    "## Exercise 2.3: Evaluation\n",
    "\n",
    "Implement the ``act`` function above and see how your trained policy performs.\n",
    "\n",
    "Try modifying the hyperparameters (learning rate, epsilon, ...) of the algorithm and compare performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 438
    },
    "id": "rP6Ok0Sbado-",
    "outputId": "58c0abdc-ffca-418f-9aee-ddfa59e59c1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished after 200 steps.\n"
     ]
    }
   ],
   "source": [
    "steps_taken = agent.run()\n",
    "print('Finished after %d steps.' % steps_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jg5wuFfWado-"
   },
   "source": [
    "# Exercise 3: Deep Q-learning (optional)\n",
    "\n",
    "As an optional exercise, you can study a deep Q-network as an alternative solution to the cart pole problem.\n",
    "\n",
    "A Q-network does not require a discretization of the state space, but as you will see, it can take quite a while to learn. You can try to run the following code in Colab and even use a GPU for training.\n",
    "\n",
    "The code below has been adapted from [here](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras/blob/master/chapter9-drl/dqn-cartpole-9.6.1.py). It uses two \"tricks\" that are helpful when training a deep Q-network:\n",
    "\n",
    "1. **Replay memory**: As decribed in the lecture slides, learning from batches of consecutive simulation samples is problematic: Consecutive samples in time are presumably highly correlated. This correlation in training batches leads to inefficient training and may bias the result. To deal with this problem, it is advisable to establish a replay memory and then sample (non-consecutive) state-action data points from this memory.\n",
    "\n",
    "2. **Target networks**: In Q-Learning, we update \"a guess with a guess\" which can lead to unstable results. The Bellman equation defines a relationship between Q(s(t), a(t)) and Q(s(t+1), a(t+1)). Altering Q(s(t), a(t)) may influence Q(s(t+1), a(t+1)) and other states nearby, leading to fluctuations in the training results. To overcome this effect, a copy of the network is used to compute Q(s(t+1), a(t+1)). This copy is not trained, but only periodically synced to the main network.\n",
    "\n",
    "More information on both measures can be found [here](https://towardsdatascience.com/deep-q-network-dqn-ii-b6bf911b6b2c).\n",
    "\n",
    "Study the code below and find the replay memory and target network implementations. Run the network (this may take a while, you may start with smaller networks at first), and check the results of the network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "U2JEvI4qado-"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "DsBc-M4Mado-"
   },
   "outputs": [],
   "source": [
    "class DQNAgent(CartPoleBase):\n",
    "    def __init__(self, num_episodes=300): # try: num_episodes=3000\n",
    "        \"\"\"DQN Agent on CartPole-v0 environment\n",
    "\n",
    "        Arguments:\n",
    "            num_episodes (int): max number of episodes to train\n",
    "        \"\"\"\n",
    "        CartPoleBase.__init__(self, num_episodes)\n",
    "        \n",
    "        self.action_space = self.env.action_space\n",
    "\n",
    "        # experience buffer\n",
    "        self.memory = []\n",
    "\n",
    "        # discount rate\n",
    "        self.gamma = 0.9\n",
    "\n",
    "        # initially 90% exploration, 10% exploitation\n",
    "        self.epsilon = 1.0\n",
    "        # iteratively applying decay til \n",
    "        # 10% exploration/90% exploitation\n",
    "        self.epsilon_min = 0.1\n",
    "        self.epsilon_decay = self.epsilon_min / self.epsilon\n",
    "        self.epsilon_decay = self.epsilon_decay ** (1. / float(500))\n",
    "\n",
    "        # Q Network weights filename\n",
    "        self.weights_file = 'dqn_cartpole.h5'\n",
    "        # Q Network for training\n",
    "        n_inputs = self.env.observation_space.shape[0]\n",
    "        n_outputs = self.action_space.n\n",
    "        self.q_model = self.build_model(n_inputs, n_outputs)\n",
    "        self.q_model.compile(loss='mse', optimizer=Adam())\n",
    "        self.q_model.summary()\n",
    "        # target Q Network\n",
    "        self.target_q_model = self.build_model(n_inputs, n_outputs)\n",
    "        # copy Q Network params to target Q Network\n",
    "        self.update_weights()\n",
    "\n",
    "        self.replay_counter = 0\n",
    "\n",
    "    \n",
    "    def build_model(self, n_inputs, n_outputs):\n",
    "        \"\"\"Q Network is 256-256-256 MLP\n",
    "\n",
    "        Arguments:\n",
    "            n_inputs (int): input dim\n",
    "            n_outputs (int): output dim\n",
    "\n",
    "        Return:\n",
    "            q_model (Model): DQN\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=(n_inputs, ), name='state')\n",
    "        x = Dense(256, activation='relu')(inputs)\n",
    "        x = Dense(256, activation='relu')(x)\n",
    "        x = Dense(256, activation='relu')(x)\n",
    "        x = Dense(n_outputs,\n",
    "                  activation='linear', \n",
    "                  name='action')(x)\n",
    "        q_model = Model(inputs, x)\n",
    "        return q_model\n",
    "\n",
    "\n",
    "    def save_weights(self):\n",
    "        \"\"\"save Q Network params to a file\"\"\"\n",
    "        self.q_model.save_weights(self.weights_file)\n",
    "\n",
    "        \n",
    "    def load_weights(self):\n",
    "        \"\"\"load Q Network params to a file\"\"\"\n",
    "        self.q_model.load_weights(self.weights_file)\n",
    "        \n",
    "        \n",
    "    def update_weights(self):\n",
    "        \"\"\"copy trained Q Network params to target Q Network\"\"\"\n",
    "        self.target_q_model.set_weights(self.q_model.get_weights())\n",
    "\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"store experiences in the replay buffer\n",
    "        Arguments:\n",
    "            state (tensor): env state\n",
    "            action (tensor): agent action\n",
    "            reward (float): reward received after executing\n",
    "                action on state\n",
    "            next_state (tensor): next state\n",
    "        \"\"\"\n",
    "        item = (state, action, reward, next_state, done)\n",
    "        self.memory.append(item)\n",
    "\n",
    "\n",
    "    def get_target_q_value(self, next_state, reward):\n",
    "        \"\"\"compute Q_max\n",
    "           Use of target Q Network solves the \n",
    "            non-stationarity problem\n",
    "        Arguments:\n",
    "            reward (float): reward received after executing\n",
    "                action on state\n",
    "            next_state (tensor): next state\n",
    "        Return:\n",
    "            q_value (float): max Q-value computed\n",
    "        \"\"\"\n",
    "        # max Q value among next state's actions\n",
    "        # DQN chooses the max Q value among next actions\n",
    "        # selection and evaluation of action is \n",
    "        # on the target Q Network\n",
    "        # Q_max = max_a' Q_target(s', a')\n",
    "        q_value = np.amax(\\\n",
    "                     self.target_q_model.predict(next_state)[0])\n",
    "\n",
    "        # Q_max = reward + gamma * Q_max\n",
    "        q_value *= self.gamma\n",
    "        q_value += reward\n",
    "        return q_value\n",
    "\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        \"\"\"experience replay addresses the correlation issue \n",
    "            between samples\n",
    "        Arguments:\n",
    "            batch_size (int): replay buffer batch \n",
    "                sample size\n",
    "        \"\"\"\n",
    "        # sars = state, action, reward, state' (next_state)\n",
    "        sars_batch = random.sample(self.memory, batch_size)\n",
    "        state_batch, q_values_batch = [], []\n",
    "\n",
    "        # fixme: for speedup, this could be done on the tensor level\n",
    "        # but easier to understand using a loop\n",
    "        for state, action, reward, next_state, done in sars_batch:\n",
    "            # policy prediction for a given state\n",
    "            q_values = self.q_model.predict(state)\n",
    "            \n",
    "            # get Q_max\n",
    "            q_value = self.get_target_q_value(next_state, reward)\n",
    "\n",
    "            # correction on the Q value for the action used\n",
    "            q_values[0][action] = reward if done else q_value\n",
    "\n",
    "            # collect batch state-q_value mapping\n",
    "            state_batch.append(state[0])\n",
    "            q_values_batch.append(q_values[0])\n",
    "\n",
    "        # train the Q-network\n",
    "        self.q_model.fit(np.array(state_batch),\n",
    "                         np.array(q_values_batch),\n",
    "                         batch_size=batch_size,\n",
    "                         epochs=1,\n",
    "                         verbose=0)\n",
    "\n",
    "        # update exploration-exploitation probability\n",
    "        self.update_epsilon()\n",
    "\n",
    "        # copy new params on old target after \n",
    "        # every 10 training updates\n",
    "        if self.replay_counter % 10 == 0:\n",
    "            self.update_weights()\n",
    "\n",
    "        self.replay_counter += 1\n",
    "\n",
    "    \n",
    "    def update_epsilon(self):\n",
    "        \"\"\"decrease the exploration, increase exploitation\"\"\"\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        \n",
    "        \n",
    "    def train(self):\n",
    "        args = {}\n",
    "        args['env_id'] = 'CartPole-v0'\n",
    "        args['no-render'] = True\n",
    "\n",
    "        # the number of trials without falling over\n",
    "        win_trials = 100\n",
    "\n",
    "        # the CartPole-v0 is considered solved if \n",
    "        # for 100 consecutive trials, he cart pole has not \n",
    "        # fallen over and it has achieved an average \n",
    "        # reward of 195.0 \n",
    "        # a reward of +1 is provided for every timestep \n",
    "        # the pole remains upright\n",
    "        win_reward = { 'CartPole-v0' : 195.0 }\n",
    "\n",
    "        # stores the reward per episode\n",
    "        scores = deque(maxlen=win_trials)\n",
    "\n",
    "        self.env.seed(0)\n",
    "\n",
    "        # should be solved in this number of episodes\n",
    "        episode_count = self.num_episodes\n",
    "        state_size = self.env.observation_space.shape[0]\n",
    "        batch_size = 64\n",
    "\n",
    "        # by default, CartPole-v0 has max episode steps = 200\n",
    "        # you can use this to experiment beyond 200\n",
    "        # env._max_episode_steps = 4000\n",
    "\n",
    "        # Q-Learning sampling and fitting\n",
    "        for episode in range(episode_count):\n",
    "            state = self.env.reset()\n",
    "            state = np.reshape(state, [1, state_size])\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            while not done:\n",
    "                # in CartPole-v0, action=0 is left and action=1 is right\n",
    "                if np.random.rand() < self.epsilon:\n",
    "                    # explore - do random action\n",
    "                    action = self.action_space.sample()\n",
    "                else:\n",
    "                    # exploit\n",
    "                    q_values = self.q_model.predict(state)\n",
    "                    # select the action with max Q-value\n",
    "                    action = np.argmax(q_values[0])\n",
    "                \n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                # in CartPole-v0:\n",
    "                # state = [pos, vel, theta, angular speed]\n",
    "                next_state = np.reshape(next_state, [1, state_size])\n",
    "                # store every experience unit in replay buffer\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "\n",
    "\n",
    "            # call experience relay\n",
    "            if len(self.memory) >= batch_size:\n",
    "                self.replay(batch_size)\n",
    "\n",
    "            scores.append(total_reward)\n",
    "            mean_score = np.mean(scores)\n",
    "            if mean_score >= win_reward[args['env_id']] \\\n",
    "                    and episode >= win_trials:\n",
    "                print(\"Solved in episode %d: \\\n",
    "                       Mean survival = %0.2lf in %d episodes\"\n",
    "                      % (episode, mean_score, win_trials))\n",
    "                print(\"Epsilon: \", self.epsilon)\n",
    "                self.save_weights()\n",
    "                break\n",
    "            if True: #(episode + 1) % win_trials == 0:\n",
    "                print(\"Episode %d: Mean survival = \\\n",
    "                       %0.2lf in %d episodes\" %\n",
    "                      ((episode + 1), mean_score, win_trials))\n",
    "                sys.stdout.flush()\n",
    "\n",
    "        # close the env and write monitor result info to disk\n",
    "        self.env.close() \n",
    "        \n",
    "        \n",
    "    def act(self, observation, reward, done):\n",
    "        \"\"\"\n",
    "        An deep Q-learning based agent.\n",
    "        \n",
    "        Arguments (everything the simulator provides): \n",
    "            observation: the observed state (Cart Position, Cart Velocity, Pole Angle, \n",
    "                         Pole Angular Velocity)\n",
    "            reward:      1 for every step taken\n",
    "            done:        true if simulation terminated\n",
    "    \n",
    "        Return:\n",
    "            0 to push cart to the left, 1 to push cart to the right\n",
    "        \"\"\"\n",
    "        state_size = self.env.observation_space.shape[0]\n",
    "        state = np.reshape(observation, [1, state_size])\n",
    "        q_values = self.q_model.predict(state)\n",
    "        # select the action with max Q-value\n",
    "        action = np.argmax(q_values[0])\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SYfd2K6Wado_",
    "outputId": "0870ab86-fe33-4edb-ba03-7f70d7224999"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "state (InputLayer)           [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               1280      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "action (Dense)               (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 133,378\n",
      "Trainable params: 133,378\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dqnagent = DQNAgent()\n",
    "dqnagent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tj8WYH2piBq2"
   },
   "outputs": [],
   "source": [
    "dqnagent.load_weights()\n",
    "dqnagent.update_weights()\n",
    "steps_taken = dqnagent.run()\n",
    "print('Finished after %d steps.' % steps_taken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MP_Assignment7_solution.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
