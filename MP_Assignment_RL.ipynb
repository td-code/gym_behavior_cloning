{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_bRzdowadou"
   },
   "source": [
    "# Motion Planning, Assignment 8: Q-Learning with openai gym\n",
    "\n",
    "Thao Dang, Hochschule Esslingen\n",
    "\n",
    "In this assignment you will experiment with Q learning for a classic control problem: balancing a pole on a moving cart.\n",
    "\n",
    "![cartpole.png](https://gymnasium.farama.org/_images/cart_pole.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8s23mvXNadoz"
   },
   "source": [
    "Note for solution: \n",
    "This notebook is based on https://github.com/IsaacPatole/CartPole-v0-using-Q-learning-SARSA-and-DNN/blob/master/Qlearning_for_cartpole.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C-1AWMGQado0"
   },
   "source": [
    "# Simulations with openai gym\n",
    "\n",
    "This assignment will utilize a famous simulation environment for testing reinforcement learning algorithms: https://gymnasium.farama.org/\n",
    "\n",
    "Pytorch will be used later, so you should run this notebook with a GPU.\n",
    "To enable GPU's in colab, select \n",
    "```bash\n",
    "\"Runtime\" > â€œChange runtime typeâ€ > \"T4 GPU\"\n",
    "```\n",
    "after you uploaded the notebook file to colab.\n",
    "\n",
    "## Test gymnasium environment\n",
    "\n",
    "First, upload the required additional files to colab (file symbol icon on the left side bar):\n",
    "- utils.py \n",
    "\n",
    "Then, install the necessary modules:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8K6hNRX4ado0"
   },
   "source": [
    "%%capture cap --no-stderr\n",
    "!pip install swig\n",
    "!pip install moviepy\n",
    "!pip install gymnasium[classic-control]\n",
    "!pip install graphviz\n",
    "!pip install torchview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m4MmSLgwado0"
   },
   "source": [
    "Now, let's import all relevant packages and define some import functions that help displaying videos on colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "238zytHoado0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "from dataclasses import dataclass, field\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from utils import *\n",
    "\n",
    "# torch.use_deterministic_algorithms(True)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "# The flag below controls whether to allow TF32 on matmul. This flag defaults to True.\n",
    "torch.backends.cuda.matmul.allow_tf32 = False\n",
    "# The flag below controls whether to allow TF32 on cuDNN. This flag defaults to True.\n",
    "torch.backends.cudnn.allow_tf32 = False\n",
    "\n",
    "# remove old video files (if any)\n",
    "if os.path.exists('output'):\n",
    "    os.system('rm -f output/*.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NqlpqrwUado2"
   },
   "source": [
    "## The cart pole environment\n",
    "\n",
    "After everything has been set up, we are ready to try some nice simulations. Here, we will only use the cart pole environment (https://gymnasium.farama.org/environments/classic_control/cart_pole/). The description of this environment is as follows (taken from: https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/classic_control/cartpole.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Description\n",
    "> \n",
    "> This environment corresponds to the version of the cart-pole problem described by Barto, Sutton, and Anderson in [\"Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problem\"](https://ieeexplore.ieee.org/document/6313077). A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces in the left and right direction on the cart.\n",
    ">\n",
    ">## Action Space\n",
    ">\n",
    ">The action is a `ndarray` with shape `(1,)` which can take values `{0, 1}` indicating the direction of the fixed force the cart is pushed with.\n",
    ">\n",
    ">- 0: Push cart to the left\n",
    ">- 1: Push cart to the right\n",
    ">\n",
    ">**Note**: The velocity that is reduced or increased by the applied force is not fixed and it depends on the angle the pole is pointing. The center of gravity of the pole varies the amount of energy needed to move the cart underneath it\n",
    ">\n",
    ">## Observation Space\n",
    ">\n",
    ">The observation is a `ndarray` with shape `(4,)` with the values corresponding to the following positions and velocities:\n",
    ">\n",
    ">| Num | Observation           | Min                 | Max               |\n",
    ">|-----|-----------------------|---------------------|-------------------|\n",
    ">| 0   | Cart Position         | -4.8                | 4.8               |\n",
    ">| 1   | Cart Velocity         | -Inf                | Inf               |\n",
    ">| 2   | Pole Angle            | ~ -0.418 rad (-24Â°) | ~ 0.418 rad (24Â°) |\n",
    ">| 3   | Pole Angular Velocity | -Inf                | Inf               |\n",
    ">\n",
    ">**Note:** While the ranges above denote the possible values for observation space of each element, it is not reflective of the allowed values of the state space in an unterminated episode. Particularly:\n",
    ">-  The cart x-position (index 0) can be take values between `(-4.8, 4.8)`, but the episode terminates if the cart leaves the `(-2.4, 2.4)` range.\n",
    ">-  The pole angle can be observed between  `(-.418, .418)` radians (or **Â±24Â°**), but the episode terminates if the pole angle is not in the range `(-.2095, .2095)` (or **Â±12Â°**)\n",
    ">\n",
    ">## Rewards\n",
    ">Since the goal is to keep the pole upright for as long as possible, by default, a reward of `+1` is given for every step taken, including the termination step. The default reward threshold is 500 for v1 and 200 for v0 due to the time limit on the environment.\n",
    ">\n",
    ">If `sutton_barto_reward=True`, then a reward of `0` is awarded for every non-terminating step and `-1` for the terminating step. As a result, the reward threshold is 0 for v0 and v1.\n",
    ">\n",
    ">## Starting State\n",
    ">All observations are assigned a uniformly random value in `(-0.05, 0.05)`\n",
    ">\n",
    ">## Episode End\n",
    ">The episode ends if any one of the following occurs:\n",
    ">\n",
    ">1. Termination: Pole Angle is greater than Â±12Â°\n",
    ">2. Termination: Cart Position is greater than Â±2.4 (center of the cart reaches the edge of the display)\n",
    ">3. Truncation: Episode length is greater than 500 (200 for v0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SCv7Wq_dado3"
   },
   "source": [
    "To run this simulation, the following class ``CartPoleBase`` may be useful. It provides two important functions:\n",
    "* ``act``: the standard policy function that generates an action from the current state data (you will overwrite this function later), and\n",
    "* ``run``: which runs and renders a simulation once ``act`` has been defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "nCa40ihBado3"
   },
   "outputs": [],
   "source": [
    "class CartPoleBase():\n",
    "    def __init__(self, max_steps=200):\n",
    "        \"\"\"\n",
    "        The constructor.\n",
    "        max_steps is the max number of steps to take (given that the trial does not fail before)\n",
    "        \"\"\"\n",
    "        self.max_steps = max_steps\n",
    "        self.env = gym.make('CartPole-v1', render_mode = \"rgb_array\")\n",
    "        self.env = RenderFrame(self.env, \"./output\")\n",
    "        \n",
    "        \n",
    "    def act(self, observation, reward, done):\n",
    "        \"\"\"\n",
    "        A very simple agent that randomly chooses an action.\n",
    "        \n",
    "        Arguments (everything the simulator provides): \n",
    "            observation: the observed state (Cart Position, Cart Velocity, Pole Angle, \n",
    "                         Pole Angular Velocity)\n",
    "            reward:      1 for every step taken\n",
    "            done:        true if simulation terminated\n",
    "    \n",
    "        Return:\n",
    "            0 to push cart to the left, 1 to push cart to the right\n",
    "        \"\"\"\n",
    "        assert False, \"not implemented yet\"\n",
    "        return 0 \n",
    "        \n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Runs an episode while displaying the cartpole environment.\"\"\"\n",
    "        t = 0\n",
    "        reward = 0\n",
    "        done = False\n",
    "        obs, _ = self.env.reset()\n",
    "        \n",
    "        while t < self.max_steps:\n",
    "            t = t+1\n",
    "            action = self.act(obs, reward, done)\n",
    "            obs, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        \n",
    "        self.env.release()\n",
    "        self.env.play()\n",
    "        return t\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4ItpmPDado3"
   },
   "source": [
    "# Exercise 1: Random policy\n",
    "\n",
    "Let's first try everything we have installed so far by running an agent that chooses actions at random and render the scene. You will only have to add one line here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "wNNPPtYlado3"
   },
   "outputs": [],
   "source": [
    "class CartPoleRandom(CartPoleBase):\n",
    "    def act(self, observation, reward, done):\n",
    "        \"\"\"\n",
    "        A very simple agent that randomly chooses an action.\n",
    "        \n",
    "        Arguments (everything the simulator provides): \n",
    "            observation: the observed state (Cart Position, Cart Velocity, Pole Angle, \n",
    "                         Pole Angular Velocity)\n",
    "            reward:      1 for every step taken\n",
    "            done:        true if simulation terminated\n",
    "    \n",
    "        Return:\n",
    "            0 to push cart to the left, 1 to push cart to the right\n",
    "        \"\"\"\n",
    "        ## TODO: Add your code here \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 438
    },
    "id": "kh0d5SX1ado4",
    "outputId": "7c8c632f-d646-43c3-da1a-89ee0efc7420"
   },
   "outputs": [],
   "source": [
    "randomAgent = CartPoleRandom()\n",
    "steps_taken = randomAgent.run()\n",
    "print('Finished after %d steps.' % steps_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1UXPXWmQado4"
   },
   "source": [
    "# Exercise 2: Q-learning\n",
    "\n",
    "Next, you will use Q-learning based on the Bellmann equation as described in the lecture to find a **Q-value function** or **Q-table** that estimates the maximum expected cumulative reward achievable\n",
    "from a given (state, action) pair.\n",
    "\n",
    "A Q-table is discrete in its nature, yet the problem we are considering here is continous since the observations that we make (cart position, cart velocity, pole angle, pole angular velocity) are continous, real numbers. To still be able to use Q-learning, we need to **discretize** our system. This is done by quantisation of all observations in separate buckets (or bins) and refering to them by the index of the bucket in which they belong.\n",
    "E.g. if we know that cart velocity is always within the range -0.5..0.5 and we want to use four bins for discretization, we could transform the real-valued cart velocity $v$ to a discrete value $v_d=0$ if $v < -0.25$, to $v_d=1$ if $-0.25 \\leq v < 0$, to $v_d=2$ if $0 \\leq v < 0.25$, and to $v_d=3$ if $v \\geq 0.5$. Using such a discretization, our cart pole problem becomes computationally tractable for the Q-learning algorithm.  \n",
    "\n",
    "The Q-learning algorithm that you will implement here is a slightly modified version from the algorithm given in the lecture:\n",
    "\n",
    "> **Given**: rewards $r$, discount factor $\\gamma$, learning rate $\\alpha$, probability threshold $\\epsilon$\n",
    ">\n",
    "> **Algorithm**:\n",
    "> 1. Initialize Q-table to zero.\n",
    "> 2. For each episode (a.k.a. training session):\n",
    ">> 3. Select (random) initial state $s_t$.\n",
    ">> 4. Do while the goal state hasn't been reached:\n",
    ">>> 5. If random number $p<\\epsilon$: Randomly select one action $a_t$ among all possible actions for the current state $s_t$. Otherwise: Select best action based on current Q-table. Both actions result in next state $s_{t+1}$.\n",
    ">>> 6. Update expected cumulative reward: $$Q(s_t, a_t) \\leftarrow (1âˆ’\\alpha) ð‘„(s_t, a_t) + \\alpha \\left[ r(s_t, a_t) + \\gamma max_{a}â¡ \\{ Q(s_{t+1}, a) \\} \\right]$$\n",
    ">>> 7. Set the next state as the current state $s_t \\leftarrow s_{t+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "leGTPpEUado4"
   },
   "source": [
    "## Exercise 2.1: Algorithm\n",
    "\n",
    "What is the major difference between the algorithm shown above and the version presented in the lecture. What are potential reasons behind this modification?\n",
    "\n",
    "In the algorithm above, it is common to adapt the learning rate and epsilon with the episodes. How and why would you modify these parameters? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YbQF7PKvado5"
   },
   "source": [
    "The following class is a skeleton for implemening the Q-learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "q_HXIjOEado5"
   },
   "outputs": [],
   "source": [
    "class CartPoleQAgent(CartPoleBase):\n",
    "    def __init__(self, num_bins=(3, 3, 6, 6), \n",
    "                 num_episodes=300, min_lr=0.1, \n",
    "                 min_epsilon=0.1, discount=1.0, decay=25):\n",
    "        \"\"\"\n",
    "        The constructor.\n",
    "        Arguments:\n",
    "            num_bins: the number of bins for discretization of all four observations\n",
    "            num_episodes: number of episodes to train\n",
    "            min_lr: minimum learning rate\n",
    "            min_epsilon: minimum epsilon value (used for sampling random actions)\n",
    "            discount: discount factor\n",
    "            decay: a decay factor used for adaptation of the learning rate and epsilon\n",
    "        \"\"\"\n",
    "        CartPoleBase.__init__(self)\n",
    "        \n",
    "        self.num_episodes = num_episodes\n",
    "        self.num_bins = num_bins\n",
    "        self.min_lr = min_lr\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.discount = discount\n",
    "        self.decay = decay\n",
    "        \n",
    "        # action-value function initialized with 0's\n",
    "        # the Q-table has shape \"num_bins[0] x num_bins[1] x num_bins[2] x num_bins[3] x 2\"\n",
    "        self.Q_table = np.zeros(self.num_bins + (self.env.action_space.n,))\n",
    "\n",
    "        # [position, velocity, angle, angular velocity]\n",
    "        self.upper_bounds = [self.env.observation_space.high[0], 0.5, self.env.observation_space.high[2], np.radians(50) / 1.]\n",
    "        self.lower_bounds = [self.env.observation_space.low[0], -0.5, self.env.observation_space.low[2], -np.radians(50) / 1.]\n",
    "        \n",
    "        # bins for discretization\n",
    "        self.bins = []\n",
    "        for low, high, num in zip(self.lower_bounds, self.upper_bounds, self.num_bins):\n",
    "            delta = (high-low)/num\n",
    "            b = [low+(i+1)*delta for i in range(num-1)]\n",
    "            self.bins.append(b)\n",
    "        \n",
    "        # book keeping of steps taken in each episode\n",
    "        self.steps = np.zeros(self.num_episodes)\n",
    "        \n",
    "\n",
    "    def discretize_state(self, obs):\n",
    "        \"\"\"\n",
    "        Takes an observation of the environment and aliases it.\n",
    "        By doing this, very similar observations can be treated\n",
    "        as the same and it reduces the state space so that the \n",
    "        Q-table can be smaller and more easily filled.\n",
    "        \n",
    "        Input:\n",
    "        obs (tuple): Tuple containing 4 floats describing the current\n",
    "                     state of the environment.\n",
    "        \n",
    "        Output:\n",
    "        discretized (tuple): Tuple containing 4 non-negative integers smaller \n",
    "                             than n where n is the number in the same position\n",
    "                             in the num_bins list.\n",
    "        \"\"\"\n",
    "        discretized = list()\n",
    "        for i in range(len(obs)):\n",
    "            new_obs = np.digitize(obs[i], self.bins[i])\n",
    "            discretized.append(new_obs)\n",
    "        \n",
    "        return tuple(discretized)\n",
    "\n",
    "\n",
    "    def decay_function(self, t, min_val):\n",
    "        \"\"\"\n",
    "        A decay function for modelling learning rate and epsilon in training function.\n",
    "        Arguments:\n",
    "            t: num epochs\n",
    "            min_val: returned decay value is never smaller than this\n",
    "        \"\"\"\n",
    "        return max(min_val, min(1., 1. - np.log10((t + 1) / self.decay)))\n",
    "\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Implements the Q-learning algorithm as described in the text above.\n",
    "        \"\"\"\n",
    "        # pause video generation\n",
    "        self.env.pause()\n",
    "        \n",
    "        # Looping for each episode\n",
    "        for e in tqdm(range(self.num_episodes)):\n",
    "            # Initializes the state\n",
    "            current_state = self.discretize_state(self.env.reset()[0])\n",
    "\n",
    "            # decaying learning rate and epsilon \n",
    "            self.learning_rate = self.decay_function(e, self.min_lr)\n",
    "            self.epsilon = self.decay_function(e, self.min_epsilon)\n",
    "            \n",
    "            # Looping for each step\n",
    "            while True:\n",
    "                self.steps[e] += 1\n",
    "                \n",
    "                ## TODO: Add your code here\n",
    "                \n",
    "\n",
    "        print('Finished training!')\n",
    "        \n",
    "        # resume video generation\n",
    "        self.env.resume()\n",
    "        \n",
    "    \n",
    "    def plot_learning(self):\n",
    "        \"\"\"\n",
    "        Plots the number of steps at each episode and prints the\n",
    "        amount of times that an episode was successfully completed.\n",
    "        \"\"\"\n",
    "        sns.lineplot(x=range(len(self.steps)), y=self.steps)\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Steps\")\n",
    "        plt.show()\n",
    "        t = 0\n",
    "        for i in range(self.num_episodes):\n",
    "            if self.steps[i] >= self.max_steps-1:\n",
    "                t+=1\n",
    "        print(t, \" episodes were successfully completed.\")\n",
    "        \n",
    "        \n",
    "    def act(self, observation, reward, done):\n",
    "        \"\"\"\n",
    "        A Q-learning based agent.\n",
    "        \n",
    "        Arguments (everything the simulator provides): \n",
    "            observation: the observed state (Cart Position, Cart Velocity, Pole Angle, \n",
    "                         Pole Angular Velocity)\n",
    "            reward:      1 for every step taken\n",
    "            done:        true if simulation terminated\n",
    "    \n",
    "        Return:\n",
    "            0 to push cart to the left, 1 to push cart to the right\n",
    "        \"\"\"\n",
    "        \n",
    "        ## TODO: Add your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ugkmBiKdado8"
   },
   "source": [
    "## Exercise 2.2: Training\n",
    "\n",
    "Implement the ``train`` function above and run the training using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "id": "_iqmA2Trado9",
    "outputId": "32bf65e0-0522-4a54-c8c5-ce4f5b41e826"
   },
   "outputs": [],
   "source": [
    "agent = CartPoleQAgent()\n",
    "agent.train()\n",
    "agent.plot_learning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jsQFxEJhado-"
   },
   "source": [
    "## Exercise 2.3: Evaluation\n",
    "\n",
    "Implement the ``act`` function above and see how your trained policy performs.\n",
    "\n",
    "Try modifying the hyperparameters (learning rate, epsilon, ...) of the algorithm and compare performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 438
    },
    "id": "rP6Ok0Sbado-",
    "outputId": "58c0abdc-ffca-418f-9aee-ddfa59e59c1c"
   },
   "outputs": [],
   "source": [
    "steps_taken = agent.run()\n",
    "print('Finished after %d steps.' % steps_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jg5wuFfWado-"
   },
   "source": [
    "# Exercise 3: Deep Q-learning (optional)\n",
    "\n",
    "As an optional exercise, you can study a deep Q-network as an alternative solution to the cart pole problem.\n",
    "\n",
    "A Q-network does not require a discretization of the state space, but as you will see, it can take quite a while to learn. You can try to run the following code in Colab and even use a GPU for training.\n",
    "\n",
    "The code below has been adapted from [here](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras/blob/master/chapter9-drl/dqn-cartpole-9.6.1.py). It uses two \"tricks\" that are helpful when training a deep Q-network:\n",
    "\n",
    "1. **Replay memory**: As decribed in the lecture slides, learning from batches of consecutive simulation samples is problematic: Consecutive samples in time are presumably highly correlated. This correlation in training batches leads to inefficient training and may bias the result. To deal with this problem, it is advisable to establish a replay memory and then sample (non-consecutive) state-action data points from this memory.\n",
    "\n",
    "2. **Target networks**: In Q-Learning, we update \"a guess with a guess\" which can lead to unstable results. The Bellman equation defines a relationship between Q(s(t), a(t)) and Q(s(t+1), a(t+1)). Altering Q(s(t), a(t)) may influence Q(s(t+1), a(t+1)) and other states nearby, leading to fluctuations in the training results. To overcome this effect, a copy of the network is used to compute Q(s(t+1), a(t+1)). This copy is not trained, but only periodically synced to the main network.\n",
    "\n",
    "More information on both measures can be found [here](https://towardsdatascience.com/deep-q-network-dqn-ii-b6bf911b6b2c).\n",
    "\n",
    "Study the code below and add a three layer fully connected network (each layer with 256 nodes and ReLU activation). \n",
    "Find the replay memory and target network implementations. Run the network (this may take a while, you may start with smaller networks at first), and check the results of the network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class DeepQLearningNetwork(torch.nn.Module):\n",
    "    def __init__(self, action_space, observation_space):\n",
    "        \"\"\"\n",
    "        Implementation of the network layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "                \n",
    "        self.n_inputs = observation_space.shape[0]\n",
    "        self.n_outputs = action_space.n\n",
    "        \n",
    "        # TODO: Add your code here\n",
    "        # Start with a simple network with 3 hidden layers of 256 neurons each and add ReLU activation functions\n",
    "\n",
    "\n",
    "    def forward(self, observation):\n",
    "        \"\"\"\n",
    "        The forward pass of the network. Returns the prediction for the given\n",
    "        input observation.\n",
    "        observation:   torch.Tensor of size (batch_size, n_inputs)\n",
    "        return         torch.Tensor of size (batch_size, n_outputs)\n",
    "        \"\"\"\n",
    "        batch_size = observation.shape[0]\n",
    "\n",
    "         # TODO: Add your code here\n",
    "\n",
    "\n",
    "    def scores_to_action(self, scores):\n",
    "        \"\"\"\n",
    "        Maps the scores predicted by the network to an action-class.\n",
    "        scores:         python list of torch.Tensors of size n_outputs\n",
    "        return          0 to push cart to the left, 1 to push cart to the right\n",
    "        \"\"\"\n",
    "        _, class_number = torch.max(scores[0], dim=0)  \n",
    "        return class_number\n",
    "\n",
    "    \n",
    "print(DeepQLearningNetwork(action_space=agent.env.action_space, observation_space=agent.env.observation_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(CartPoleBase):\n",
    "    def __init__(self, num_episodes=500):\n",
    "        \"\"\"DQN Agent on CartPole-v0 environment\n",
    "\n",
    "        Arguments:\n",
    "            num_episodes (int): max number of episodes to train\n",
    "        \"\"\"\n",
    "        CartPoleBase.__init__(self)\n",
    "       \n",
    "        self.num_episodes = num_episodes \n",
    "        self.action_space = self.env.action_space\n",
    "\n",
    "        # experience buffer\n",
    "        self.memory = []\n",
    "\n",
    "        # discount rate\n",
    "        self.gamma = 0.9\n",
    "\n",
    "        # initially 90% exploration, 10% exploitation\n",
    "        self.epsilon = 1.0\n",
    "        # iteratively applying decay til \n",
    "        # 10% exploration/90% exploitation\n",
    "        self.epsilon_min = 0.1\n",
    "        self.epsilon_decay = self.epsilon_min / self.epsilon\n",
    "        self.epsilon_decay = self.epsilon_decay ** (1. / float(500))\n",
    "\n",
    "        # Q Network weights filename\n",
    "        self.weights_file = \"agent.pth\"\n",
    "        \n",
    "        # Q Network for training\n",
    "        self.q_model = DeepQLearningNetwork(action_space=agent.env.action_space, observation_space=agent.env.observation_space)\n",
    "\n",
    "        self.learning_rate = 0.001\n",
    "        self.optimizer = torch.optim.Adam(self.q_model.parameters(), lr=self.learning_rate)\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "        # target Q Network\n",
    "        self.target_q_model = DeepQLearningNetwork(action_space=agent.env.action_space, observation_space=agent.env.observation_space)\n",
    "        # copy Q Network params to target Q Network\n",
    "        self.update_weights()\n",
    "\n",
    "        self.replay_counter = 0\n",
    "\n",
    "    def save_weights(self):\n",
    "        \"\"\"save Q Network params to a file\"\"\"\n",
    "        torch.save(self.q_model, self.weights_file)\n",
    "        \n",
    "    def load_weights(self):\n",
    "        \"\"\"load Q Network params to a file\"\"\"\n",
    "        self.q_model = torch.load(self.weights_file, map_location=device)        \n",
    "        \n",
    "    def update_weights(self):\n",
    "        \"\"\"copy trained Q Network params to target Q Network\"\"\"\n",
    "        self.target_q_model.load_state_dict(self.q_model.state_dict())\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"store experiences in the replay buffer\n",
    "        Arguments:\n",
    "            state (tensor): env state\n",
    "            action (tensor): agent action\n",
    "            reward (float): reward received after executing action on state\n",
    "            next_state (tensor): next state\n",
    "        \"\"\"\n",
    "        item = (state, action, reward, next_state, done)\n",
    "        self.memory.append(item)\n",
    "\n",
    "    def get_target_q_value(self, next_state, reward):\n",
    "        \"\"\"compute Q_max\n",
    "           Use of target Q Network solves the non-stationarity problem\n",
    "        Arguments:\n",
    "            reward (float): reward received after executing  action on state\n",
    "            next_state (tensor): next state\n",
    "        Return:\n",
    "            q_value (float): max Q-value computed\n",
    "        \"\"\"\n",
    "        # max Q value among next state's actions\n",
    "        # DQN chooses the max Q value among next actions\n",
    "        # selection and evaluation of action is on the target Q Network\n",
    "        # Q_max = max_a' Q_target(s', a')\n",
    "        q_values = self.target_q_model(torch.Tensor(next_state).to(device))\n",
    "        q_value = torch.max(q_values).item()\n",
    "\n",
    "        # Q_max = reward + gamma * Q_max\n",
    "        q_value *= self.gamma\n",
    "        q_value += reward\n",
    "        return q_value\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        \"\"\"experience replay addresses the correlation issue between samples\n",
    "        Arguments:\n",
    "            batch_size (int): replay buffer batch sample size\n",
    "        \"\"\"\n",
    "        # sars = state, action, reward, state' (next_state)\n",
    "        sars_batch = random.sample(self.memory, batch_size)\n",
    "        state_batch, q_values_batch = [], []\n",
    "\n",
    "        # fixme: for speedup, this could be done on the tensor level\n",
    "        # but easier to understand using a loop\n",
    "        for state, action, reward, next_state, done in sars_batch:\n",
    "            # policy prediction for a given state\n",
    "            q_values = self.target_q_model(torch.Tensor(state).to(device))\n",
    "\n",
    "            # get Q_max\n",
    "            q_value = self.get_target_q_value(next_state, reward)\n",
    "\n",
    "            # correction on the Q value for the action used\n",
    "            q_values[0][action] = reward if done else q_value\n",
    "\n",
    "            # collect batch state-q_value mapping\n",
    "            state_batch.append(torch.tensor(state).to(device))\n",
    "            q_values_batch.append(torch.tensor(q_values).to(device))\n",
    "\n",
    "        # train the Q-network\n",
    "        self.q_model.train()\n",
    "        batch_in = torch.reshape(torch.cat(state_batch, dim=0), (-1, self.q_model.n_inputs))\n",
    "        batch_gt = torch.reshape(torch.cat(q_values_batch, dim=0), (-1, self.q_model.n_outputs))\n",
    "\n",
    "        assert batch_in.shape[0] == batch_size\n",
    "        assert batch_in.shape[1] == self.q_model.n_inputs\n",
    "        assert batch_gt.shape[0] == batch_size\n",
    "        assert batch_gt.shape[1] == self.q_model.n_outputs\n",
    "        \n",
    "        batch_out = self.q_model(batch_in)\n",
    "        loss = self.loss_fn(batch_out, batch_gt)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.q_model.eval()\n",
    "\n",
    "        # update exploration-exploitation probability\n",
    "        self.update_epsilon()\n",
    "\n",
    "        # copy new params on old target after every 10 training updates\n",
    "        if self.replay_counter % 10 == 0:\n",
    "            self.update_weights()\n",
    "\n",
    "        self.replay_counter += 1\n",
    "\n",
    "    \n",
    "    def update_epsilon(self):\n",
    "        \"\"\"decrease the exploration, increase exploitation\"\"\"\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        \n",
    "        \n",
    "    def train(self):\n",
    "        # pause video generation\n",
    "        self.env.pause()\n",
    "        \n",
    "        # the number of trials without falling over\n",
    "        win_trials = 100\n",
    "\n",
    "        # the CartPole is considered solved if \n",
    "        # for 100 consecutive trials, he cart pole has not \n",
    "        # fallen over and it has achieved an average \n",
    "        # reward of 195.0 \n",
    "        # a reward of +1 is provided for every timestep \n",
    "        # the pole remains upright\n",
    "        win_reward = 195.0\n",
    "\n",
    "        # stores the reward per episode\n",
    "        scores = deque(maxlen=win_trials)\n",
    "\n",
    "        state, _ = self.env.reset(seed=0)\n",
    "\n",
    "        # should be solved in this number of episodes\n",
    "        episode_count = self.num_episodes\n",
    "        state_size = self.env.observation_space.shape[0]\n",
    "        batch_size = 64\n",
    "        \n",
    "        self.q_model.eval()\n",
    "\n",
    "        # by default, CartPole-v0 has max episode steps = 200\n",
    "        # you can use this to experiment beyond 200\n",
    "        # env._max_episode_steps = 4000\n",
    "\n",
    "        # Q-Learning sampling and fitting\n",
    "        for episode in tqdm(range(episode_count)):\n",
    "            state, _ = self.env.reset()\n",
    "            state = np.reshape(state, [1, state_size])\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            while not done:\n",
    "                # in CartPole-v1, action=0 is left and action=1 is right\n",
    "                if np.random.rand() < self.epsilon:\n",
    "                    # explore - do random action\n",
    "                    action = self.action_space.sample()\n",
    "                else:\n",
    "                    # exploit\n",
    "                    q_values = self.q_model(torch.Tensor(state).to(device))\n",
    "                    # select the action with max Q-value\n",
    "                    action = torch.argmax(q_values, dim=1).item()\n",
    "                \n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                done = terminated or truncated\n",
    "                \n",
    "                # in CartPole-v1:\n",
    "                # state = [pos, vel, theta, angular speed]\n",
    "                next_state = np.reshape(next_state, [1, state_size])\n",
    "                # store every experience unit in replay buffer\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "\n",
    "\n",
    "            # call experience replay\n",
    "            if len(self.memory) >= batch_size:\n",
    "                self.replay(batch_size)\n",
    "\n",
    "            scores.append(total_reward)\n",
    "            mean_score = np.mean(scores)\n",
    "            if mean_score >= win_reward and episode >= win_trials:\n",
    "                print(\"Solved in episode %d: Mean survival = %0.2lf in %d episodes\" % (episode, mean_score, win_trials))\n",
    "                print(\"Epsilon: \", self.epsilon)\n",
    "                break\n",
    "            if (episode + 1) % win_trials == 0:\n",
    "                tqdm.write(\"Episode %d: Mean survival = %0.2lf in %d episodes\" %  ((episode + 1), mean_score, win_trials))\n",
    "\n",
    "        # close the env \n",
    "        self.env.close()\n",
    "        \n",
    "        # resume video generation\n",
    "        self.env.resume()         \n",
    "        \n",
    "        \n",
    "    def act(self, observation, reward, done):\n",
    "        \"\"\"\n",
    "        An deep Q-learning based agent.\n",
    "        \n",
    "        Arguments (everything the simulator provides): \n",
    "            observation: the observed state (Cart Position, Cart Velocity, Pole Angle, \n",
    "                         Pole Angular Velocity)\n",
    "            reward:      1 for every step taken\n",
    "            done:        true if simulation terminated\n",
    "    \n",
    "        Return:\n",
    "            0 to push cart to the left, 1 to push cart to the right\n",
    "        \"\"\"\n",
    "        state_size = self.env.observation_space.shape[0]\n",
    "        state = np.reshape(observation, [1, state_size])\n",
    "        q_values = self.target_q_model(torch.Tensor(state).to(device))\n",
    "        # select the action with max Q-value\n",
    "        action = torch.argmax(q_values, dim=1).item()\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run training untill either a max number of iterations is reached or until the performance in the last 100 trials was good enough. You may need to adjust the max number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqnagent = DQNAgent()\n",
    "dqnagent.train()\n",
    "dqnagent.save_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, run sample trials with the trained network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tj8WYH2piBq2"
   },
   "outputs": [],
   "source": [
    "dqnagent.load_weights()\n",
    "dqnagent.update_weights()\n",
    "steps_taken = dqnagent.run()\n",
    "print('Finished after %d steps.' % steps_taken)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MP_Assignment7_solution.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "behavior-cloning",
   "language": "python",
   "name": "behavior-cloning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
